{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "# from peft import (\n",
    "#     LoraConfig,\n",
    "#     prepare_model_for_kbit_training,\n",
    "#     get_peft_model\n",
    "# )\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import json\n",
    "with open('/mnt/bn/data-tns-live-llm/leon/experiments/llm/audioslice/20240103/dataset_train_pos_tcs_neg_pp/data', 'r') as f:\n",
    "    json_str = f.read()\n",
    "\n",
    "json_str = \"[\"+json_str.replace('\\n', ',')[:-1]+\"]\"\n",
    "json_str = json.loads(json_str)\n",
    "print(len(json_str))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(json_str)\n",
    "print(df.columns)\n",
    "data = df[[\"label\",\"text\"]]\n",
    "print(data.columns)\n",
    "print(data.head)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/bn/data-tns-algo-nlp/pretrain_models/mistral\")\n",
    "# tokenize the text feature \n",
    "tokenized_feature_raw = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            df.text.values.tolist(), \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True      \n",
    "                   )\n",
    "# collect tokenized sentence length \n",
    "token_sentence_length = [len(x) for x in tokenized_feature_raw['input_ids']]\n",
    "print('max: ', max(token_sentence_length))\n",
    "print('min: ', min(token_sentence_length))\n",
    "# plot the distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.hist(token_sentence_length, rwidth = 0.9)\n",
    "plt.xlabel('Sequence Length', fontsize = 18)\n",
    "plt.ylabel('# of Samples', fontsize = 18)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "# identify features and target\n",
    "features = df.text.values.tolist()\n",
    "target = df.label.values.tolist()\n",
    "#target 本身是字符列表\n",
    "target = [int(value) for value in target]\n",
    "# mistral 没有padding的token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenize features \n",
    "MAX_LEN = 256\n",
    "tokenized_feature = tokenizer.batch_encode_plus(\n",
    "                            # Sentences to encode\n",
    "                            features, \n",
    "                            # Add '[CLS]' and '[SEP]'\n",
    "                            add_special_tokens = True,\n",
    "                            # Add empty tokens if len(text)<MAX_LEN\n",
    "                            padding = 'max_length',\n",
    "                            # Truncate all sentences to max length\n",
    "                            truncation=True,\n",
    "                            # Set the maximum length\n",
    "                            max_length = MAX_LEN, \n",
    "                            # Return attention mask\n",
    "                            return_attention_mask = True,\n",
    "                            # Return pytorch tensors\n",
    "                            return_tensors = 'pt'       \n",
    "                   )\n",
    "# Use 80% for training and 20% for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(tokenized_feature['input_ids'], \n",
    "                                                                                                             target,\n",
    "                                                                                                                    tokenized_feature['attention_mask'],\n",
    "                                                                                                      random_state=2018, test_size=0.2, stratify=target)\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# define batch_size\n",
    "batch_size = 8\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# Create the DataLoader for our test set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, torch.tensor(validation_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "# BertForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification\n",
    "# model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "#     \"xlm-roberta-large\", \n",
    "#     # Specify number of classes\n",
    "#     num_labels = len(set(target)), \n",
    "#     # Whether the model returns attentions weights\n",
    "#     output_attentions = False,\n",
    "#     # Whether the model returns all hidden-states \n",
    "#     output_hidden_states = False\n",
    "# )\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# qunatization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"/mnt/bn/data-tns-algo-nlp/pretrain_models/mistral/models--mistralai--Mistral-7B-v0.1/snapshots/26bca36bde8333b5d7f72e9ed20ccda6a618af24/\", \n",
    "    # Specify number of classes\n",
    "    num_labels = len(set(target)), \n",
    "    # Whether the model returns attentions weights\n",
    "    output_attentions = False,\n",
    "    # Whether the model returns all hidden-states \n",
    "    output_hidden_states = False,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(model)\n",
    "model.print_trainable_parameters()\n",
    "# output: trainable params: 786,432\n",
    "#      || all params: 331,982,848\n",
    "#      || trainable%: 0.2368893467652883\n",
    "# Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "#————————————————超参————————————————————#\n",
    "eval_step = 1\n",
    "epochs = 50\n",
    "#————————————————超参————————————————————#\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# Number of training epochs\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps = 0,\n",
    "#                                             num_training_steps = total_steps)\n",
    "scheduler = CosineAnnealingLR(optimizer, total_steps)\n",
    "# tell pytorch to run this model on GPU\n",
    "# import torch.nn as nn\n",
    "# import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "# torch.distributed.init_process_group(backend='nccl')\n",
    "# # model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.cuda()\n",
    "# model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n",
    "\n",
    "print(model.named_parameters())\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "# compute metrics\n",
    "import numpy as np\n",
    "def compute_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    计算给定 logits 和 labels 的准确率、召回率和正负例的平均置信度。\n",
    "    :param logits: PyTorch 张量，表示模型的输出 logits。\n",
    "    :param labels: PyTorch 张量，表示真实的标签。\n",
    "    :return: 包含准确率、召回率和正负例平均置信度的元组 (accuracy, recall, pos_prob, neg_prob)。\n",
    "    \"\"\"\n",
    "    def confusion_matrix(preds, labels):\n",
    "        \"\"\"\n",
    "        计算混淆矩阵及其四个基本指标：真阳性、假阳性、真阴性和假阴性\n",
    "        \"\"\"\n",
    "        tp = torch.sum((preds == 1) & (labels == 1)).float()\n",
    "        fp = torch.sum((preds == 1) & (labels == 0)).float()\n",
    "        tn = torch.sum((preds == 0) & (labels == 0)).float()\n",
    "        fn = torch.sum((preds == 0) & (labels == 1)).float()\n",
    "        return tp, fp, tn, fn\n",
    "\n",
    "    def compute_accuracy(tp, fp, tn, fn):\n",
    "        \"\"\"\n",
    "        计算准确率\n",
    "        \"\"\"\n",
    "        total = tp + fp + tn + fn\n",
    "        acc = (tp + tn) / total\n",
    "        return acc\n",
    "\n",
    "    def compute_recall(tp, fp, tn, fn):\n",
    "        \"\"\"\n",
    "        计算召回率\n",
    "        \"\"\"\n",
    "        if tp+fn:\n",
    "            recall = tp / (tp + fn)\n",
    "        else:\n",
    "            recall = 0\n",
    "        assert recall == 0 or (not torch.isnan(recall))\n",
    "        return recall\n",
    "\n",
    "    def calc_accuracy_recall(logits, labels):\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # print(f\"preds:{preds}\")\n",
    "        # print(f\"labels:{labels}\")\n",
    "        tp, fp, tn, fn = confusion_matrix(preds, labels)\n",
    "        # print(f\"tp {tp} fp{fp} tn{tn} fn{fn}\")\n",
    "        assert tp+fp+tn+fn==labels.shape[0]\n",
    "        acc = compute_accuracy(tp, fp, tn, fn)\n",
    "        recall = compute_recall(tp, fp, tn, fn)\n",
    "        # print(f\"acc {acc} recall {recall}\")\n",
    "\n",
    "        if not isinstance(recall, int): recall = recall.item()\n",
    "        acc = acc.item()\n",
    "        return acc, recall\n",
    "\n",
    "    def calc_confidence(logits, labels):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        # print(f\"probs:{probs}\")\n",
    "        confidence_pos = probs[:, 1][labels == 1].mean().item()\n",
    "        confidence_neg = probs[:, 0][labels == 0].mean().item()\n",
    "        if np.isnan(confidence_pos): confidence_pos=0\n",
    "        if np.isnan(confidence_neg): confidence_neg=0\n",
    "        assert not (np.isnan(confidence_pos) or np.isnan(confidence_neg))\n",
    "        return confidence_pos, confidence_neg\n",
    "    \n",
    "    accuracy, recall = calc_accuracy_recall(logits, labels)\n",
    "    confidence_pos, confidence_neg = calc_confidence(logits, labels)\n",
    "    return (accuracy, recall, confidence_pos, confidence_neg)\n",
    "import torch.nn.functional as F\n",
    "focalloss_gamma = 2\n",
    "focalloss_alpha = 0.25\n",
    "def compute_loss(model, input_ids, input_mask, labels, return_outputs=False):\n",
    "        outputs = model(input_ids, attention_mask=input_mask, labels=labels)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        targets = F.one_hot(labels, num_classes=logits.shape[-1])\n",
    "        \n",
    "        p = torch.sigmoid(logits)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets.float(), reduction=\"none\")\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)\n",
    "        loss = ce_loss * ((1 - p_t) ** focalloss_gamma)\n",
    "\n",
    "        if focalloss_alpha >= 0:\n",
    "            alpha_t = focalloss_alpha * targets + (1 - focalloss_alpha) * (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "\n",
    "        loss = loss.mean()\n",
    "        # if focalloss_reduction == \"mean\":\n",
    "        #     loss = loss.mean()\n",
    "        # elif focalloss_reduction == \"sum\":\n",
    "        #     loss = loss.sum()\n",
    "\n",
    "        return loss, outputs\n",
    "\n",
    "def evaluate(model, validation_dataloader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\")\n",
    "    # Initialize lists to store metrics\n",
    "    val_loss_values = []\n",
    "    val_accuracy_values = []\n",
    "    val_recall_values = []\n",
    "    val_confidence_pos_values = []\n",
    "    val_confidence_neg_values = []\n",
    "\n",
    "    # Loop through batches of the validation dataset\n",
    "    for batch in validation_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Perform forward pass\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        # Compute loss\n",
    "        loss, outputs = compute_loss(model,b_input_ids,b_input_mask,b_labels)\n",
    "        # loss = outputs[0].mean()\n",
    "\n",
    "        val_loss_values.append(loss.item())\n",
    "\n",
    "        # Calculate epoch validation accuracy, recall, and positive and negative confidence values\n",
    "        logits = outputs[1]\n",
    "        accuracy, recall, confidence_pos, confidence_neg = compute_metrics(logits, b_labels)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        val_accuracy_values.append(accuracy)\n",
    "        val_recall_values.append(recall)\n",
    "        val_confidence_pos_values.append(confidence_pos)\n",
    "        val_confidence_neg_values.append(confidence_neg)\n",
    "\n",
    "    # Calculate average epoch validation loss\n",
    "    val_loss = np.mean(val_loss_values)\n",
    "    val_accuracy = np.mean(val_accuracy_values)\n",
    "    val_recall = np.mean(val_recall_values)\n",
    "    val_pos_prob = np.mean(val_confidence_pos_values)\n",
    "    val_neg_prob = np.mean(val_confidence_neg_values)\n",
    "\n",
    "    return val_loss, val_accuracy, val_recall, val_pos_prob, val_neg_prob\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Training\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import transformers\n",
    "\n",
    "writer = SummaryWriter(\"/opt/tiger/leon/runs/\")\n",
    "device = torch.device(\"cuda\")\n",
    "# Store the average loss after each epoch \n",
    "loss_values = []\n",
    "# number of total steps for each epoch\n",
    "print('total steps per epoch: ',  len(train_dataloader) / batch_size)\n",
    "\n",
    "# 只训练分类头\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"classifier\" not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# Train the model for each epoch\n",
    "\n",
    "best_val_recall = 0\n",
    "for epoch_i in range(epochs):\n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    train_accuracy_values = []\n",
    "    train_recall_values = []\n",
    "    train_confidence_pos_values = []\n",
    "    train_confidence_neg_values = []\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize lists to store training loss and predictions for each batch\n",
    "    train_loss_values = []\n",
    "    train_preds = []\n",
    "\n",
    "    # Get the current learning rate\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Loop through batches of the training dataset using tqdm for progress tracking\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch_i+1}/{epochs}\", unit=\"batch\")\n",
    "    # cnt = 0\n",
    "    for batch in progress_bar:\n",
    "        # cnt += 1 \n",
    "        # Load batch to GPU\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        # Compute loss\n",
    "        # loss = outputs[0]\n",
    "        loss, outputs = compute_loss(model,b_input_ids,b_input_mask,b_labels)\n",
    "        \n",
    "        train_loss_values.append(loss.item())\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record predictions\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Calculate epoch training accuracy, recall, and positive and negative confidence values\n",
    "        accuracy, recall, confidence_pos, confidence_neg = compute_metrics(logits, b_labels)\n",
    "        # Append metrics to lists\n",
    "        train_accuracy_values.append(accuracy)\n",
    "        train_recall_values.append(recall)\n",
    "        train_confidence_pos_values.append(confidence_pos)\n",
    "        train_confidence_neg_values.append(confidence_neg)\n",
    "        \n",
    "        # if cnt == 10: break\n",
    "        # Print metrics for this epoch\n",
    "        # print(f\"batch: training loss: {loss:.4f}; accuracy: {accuracy:.4f}; recall: {recall:.4f}; pos confidence: {pos_prob:.4f}; neg confidence: {neg_prob:.4f}\")\n",
    "\n",
    "    # Calculate average epoch training loss\n",
    "    train_loss = np.mean(train_loss_values)\n",
    "    accuracy = np.mean(train_accuracy_values)\n",
    "    recall = np.mean(train_recall_values)\n",
    "    pos_prob = np.mean(train_confidence_pos_values)\n",
    "    neg_prob = np.mean(train_confidence_neg_values)\n",
    "\n",
    "    # Write training metrics to TensorBoard\n",
    "    writer.add_scalar(\"Training Loss\", loss, epoch_i)\n",
    "    writer.add_scalar(\"Training Accuracy\", accuracy, epoch_i)\n",
    "    writer.add_scalar(\"Training Recall\", recall, epoch_i)\n",
    "    writer.add_scalar(\"Positive Confidence\", confidence_pos, epoch_i)\n",
    "    writer.add_scalar(\"Negative Confidence\", confidence_neg, epoch_i)\n",
    "    writer.add_scalar(\"Learning Rate\", lr, epoch_i)\n",
    "\n",
    "    # Evaluate the model on the validation set every `eval_step` epochs\n",
    "    if (epoch_i + 1) % eval_step == 0:\n",
    "        val_loss, val_accuracy, val_recall, val_confidence_pos, val_confidence_neg = evaluate(model, validation_dataloader)\n",
    "        print(f\"Epoch {epoch_i+1}/{epochs} validation loss: {val_loss:.4f}; accuracy: {val_accuracy:.4f}; recall: {val_recall:.4f}; pos confidence: {val_confidence_pos:.4f}; neg confidence: {val_confidence_neg:.4f}\")\n",
    "        # Write validation metrics to TensorBoard\n",
    "        writer.add_scalar(\"Validation Loss\", val_loss, epoch_i)\n",
    "        writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch_i)\n",
    "        writer.add_scalar(\"Validation Recall\", val_recall, epoch_i)\n",
    "        writer.add_scalar(\"Validation Positive Confidence\", val_confidence_pos, epoch_i)\n",
    "        writer.add_scalar(\"Validation Negative Confidence\", val_confidence_neg, epoch_i)\n",
    "\n",
    "        if val_recall > best_val_recall:\n",
    "            model.save_pretrained(\"best_model\")\n",
    "            best_val_recall = val_recall\n",
    "    # Print metrics for this epoch\n",
    "    print(f\"Epoch {epoch_i+1}/{epochs} training loss: {train_loss:.4f}; accuracy: {accuracy:.4f}; recall: {recall:.4f}; pos confidence: {pos_prob:.4f}; neg confidence: {neg_prob:.4f}\")\n",
    "\n",
    "# outputs 可视化\n",
    "cnt = 0\n",
    "for batch in train_dataloader:\n",
    "    cnt +=1\n",
    "    # print(batch)\n",
    "    print(batch[0].shape) #input\n",
    "    print(batch[1].shape) #mask\n",
    "    print(batch[2].shape) #label\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    print(outputs[0]) #loss\n",
    "    print(outputs[1]) #logits\n",
    "    logits = outputs[1]\n",
    "    # Calculate epoch training accuracy, recall, and positive and negative confidence values\n",
    "    accuracy, recall, confidence_pos, confidence_neg = compute_metrics(logits, b_labels)\n",
    "    # accuracy, recall, pos_prob, neg_prob = compute_accuracy_recall(logits, b_labels)\n",
    "    print(f\"training loss: {outputs[0]:.4f}; accuracy: {accuracy:.4f}; recall: {recall:.4f}; pos confidence: {confidence_pos:.4f}; neg confidence: {confidence_neg:.4f}\")\n",
    "    if cnt == 10: break\n",
    "from sklearn.metrics import f1_score\n",
    "# define custom batch preprocessor\n",
    "def collate_fn(batch, tokenizer):\n",
    "    dict_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n",
    "    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['attention_mask'], batch_first=True, padding_value=0\n",
    "    )\n",
    "    d['labels'] = torch.stack(d['labels'])\n",
    "    return d\n",
    "\n",
    "# define which metrics to compute for evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    f1_micro = f1_score(labels, predictions > 0, average = 'micro')\n",
    "    f1_macro = f1_score(labels, predictions > 0, average = 'macro')\n",
    "    f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')\n",
    "    return {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "# create custom trainer class to be able to pass label weights and calculate mutilabel loss\n",
    "class CustomTrainer(Trainer):    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        targets = F.one_hot(labels, num_classes=logits.shape[-1])\n",
    "        \n",
    "        p = torch.sigmoid(logits)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets.float(), reduction=\"none\")\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)\n",
    "        loss = ce_loss * ((1 - p_t) ** focalloss_gamma)\n",
    "\n",
    "        if focalloss_alpha >= 0:\n",
    "            alpha_t = focalloss_alpha * targets + (1 - focalloss_alpha) * (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "\n",
    "        loss = loss.mean()\n",
    "        # if focalloss_reduction == \"mean\":\n",
    "        #     loss = loss.mean()\n",
    "        # elif focalloss_reduction == \"sum\":\n",
    "        #     loss = loss.sum()\n",
    "\n",
    "        return loss, outputs\n",
    "import functools\n",
    "#define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'multilabel_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 10,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = validation_data,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
